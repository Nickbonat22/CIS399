{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"knn_w19.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"70rnwGhkbgrZ","colab_type":"text"},"cell_type":"markdown","source":["<h1>\n","<center>\n","Module 8: K-NN\n","</center>\n","</h1>\n","<div class=h1_cell>\n","<p>\n","I kind of like K Nearest Neighbor (K-NN). It is a version of crowd-sourcing like random forests. The general idea is that you choose K to be how many \"consultants\" you take votes from. With random forests, the consultants were decision trees. In K-NN, they are the other rows in the table. So let's say you are trying to predict what value to give for a row i. What you do is check all the other rows (excluding i) to get their distance from i. You choose the K closest. You take the predictions from each of those K. Majority wins and is the prediction you use for row i.\n","<p>\n","Notice anything interesting? We are not doing any training! We do have to have labeled data to get the K consultants. But no training step involved. Wow.\n","<p>\n","However, there is always a catch. Think about what we are doing. For every row, we are calculating distance to every other row. Feels `O(n)` to me. And remember, the shelter data we looked at had 26K rows! What is cost of decision tree or random forest to make a prediction? Worst case it is the depth of the tree, i.e., it is constant speed. If we need to make real-time predictions then `O(n)` might not be good enough.\n","</div>\n","<p>\n","  And btw, when testing K-NN, our cost will balloon to `O(n**2)`."]},{"metadata":{"id":"Yel61z0Pbgrb","colab_type":"code","outputId":"7c93b965-7d96-49b3-ae79-8c12778a2821","executionInfo":{"status":"ok","timestamp":1551240455620,"user_tz":480,"elapsed":335,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import pandas as pd\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"SSFKdg07gh2I","colab_type":"code","colab":{}},"cell_type":"code","source":["with open('/content/gdrive/My Drive/class_tables/loan_table_week4.csv', 'r') as f:\n","  loan_table = pd.read_csv(f)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k8aUB0VpgqXA","colab_type":"code","colab":{}},"cell_type":"code","source":["!rm library_w19_week6b.py"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dnHr-TA6gvK5","colab_type":"code","outputId":"3e0c3c08-c965-4657-d5ed-3dcf6041a633","executionInfo":{"status":"ok","timestamp":1551240470632,"user_tz":480,"elapsed":6056,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":108}},"cell_type":"code","source":["from google.colab import files\n","files.upload()"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-a47ee65a-fa5a-4d37-8d57-8d3d5d8f9d09\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-a47ee65a-fa5a-4d37-8d57-8d3d5d8f9d09\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving library_w19_week6b.py to library_w19_week6b.py\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'library_w19_week6b.py': b'import pandas as pd\\nimport numpy as np\\nfrom functools import reduce\\nfrom types import SimpleNamespace\\nimport random\\n\\ndef predictor_case(row, pred, target):\\n\\tcase_dict = {(0,0): \\'true_negative\\', (1,1): \\'true_positive\\', (0,1): \\'false_negative\\', (1,0): \\'false_positive\\'}\\n\\tactual = row[target]\\n\\tprediction = row[pred]\\n\\tcase = case_dict[(prediction, actual)]\\n\\treturn case\\n\\ndef accuracy(cases):\\n\\n    tp = 0\\n    if \\'true_positive\\' in cases:\\n        tp = cases[\\'true_positive\\']\\n    tn = 0\\n    if \\'true_negative\\' in cases:\\n        tn = cases[\\'true_negative\\']\\n    fp = 0\\n    if \\'false_positive\\' in cases:\\n        fp = cases[\\'false_positive\\']\\n    fn = 0\\n    if \\'false_negative\\' in cases:\\n        fn = cases[\\'false_negative\\']\\n\\n    if(tp+tn+fp+fn) == 0:\\n        return 0\\n\\n    return (tp + tn)/(tp+tn+fp+fn)\\n\\n#accuracy(p1_types)\\n\\ndef f1(cases):\\n\\n    #the heart of the matrix\\n    #the heart of the matrix\\n    tp = 0\\n    if \\'true_positive\\' in cases:\\n        tp = cases[\\'true_positive\\']\\n    tn = 0\\n    if \\'true_negative\\' in cases:\\n        tn = cases[\\'true_negative\\']\\n    fp = 0\\n    if \\'false_positive\\' in cases:\\n        fp = cases[\\'false_positive\\']\\n    fn = 0\\n    if \\'false_negative\\' in cases:\\n        fn = cases[\\'false_negative\\']\\n\\n    #other measures we can derive\\n    if (((tp+fn) == 0) or ((tp+fp) == 0)):\\n        return 0\\n    else:\\n        recall = 1.0*tp/(tp+fn)\\n        precision = 1.0*tp/(tp+fp)\\n\\n    #now for the one we want\\n    if ((recall == 0) or (precision == 0)):\\n        return 0\\n    else:\\n        f1 = 2/(1/recall + 1/precision)\\n\\n    return f1\\n\\ndef informedness(cases):\\n\\n    tp = 0\\n    if \\'true_positive\\' in cases:\\n        tp = cases[\\'true_positive\\']\\n    tn = 0\\n    if \\'true_negative\\' in cases:\\n        tn = cases[\\'true_negative\\']\\n    fp = 0\\n    if \\'false_positive\\' in cases:\\n        fp = cases[\\'false_positive\\']\\n    fn = 0\\n    if \\'false_negative\\' in cases:\\n        fn = cases[\\'false_negative\\']\\n    if (((tp+fn) == 0) or ((tn+fp) == 0)):\\n        return 0\\n    else:\\n        recall = 1.0*tp/(tp+fn)\\n        specificity = 1.0*tn/(tn+fp)\\n        J = (recall + specificity) - 1\\n\\n    return J\\n\\ndef gig(starting_table, split_column, target_column):\\n\\n    #split into two branches, i.e., two sub-tables\\n    true_table = starting_table.loc[starting_table[split_column] == 1]\\n    false_table = starting_table.loc[starting_table[split_column] == 0]\\n\\n    #Now see how the target column is divided up in each sub-table (and the starting table)\\n    true_counts = true_table[target_column].value_counts()  # Note using true_table and not starting_table\\n    false_counts = false_table[target_column].value_counts()  # Note using false_table and not starting_table\\n    starting_counts = starting_table[target_column].value_counts()\\n\\n    #compute the gini impurity for the 3 tables\\n    starting_gini = gini(starting_counts)\\n    true_gini = gini(true_counts)\\n    false_gini = gini(false_counts)\\n\\n    #compute the weights\\n    starting_size = len(starting_table.index)\\n    true_weight = 0.0 if starting_size == 0 else len(true_table.index)/starting_size\\n    false_weight = 0.0 if starting_size == 0 else len(false_table.index)/starting_size\\n\\n    #wrap it up and put on a bow\\n    gig = starting_gini - (true_weight * true_gini + false_weight * false_gini)\\n\\n    return gig\\n\\ndef gini(counts):\\n    (p0,p1) = probabilities(counts)\\n    sum_probs = p0**2 + p1**2\\n    gini = 1 - sum_probs\\n    return gini\\n\\ndef probabilities(counts):\\n    count_0 = 0 if 0 not in counts else counts[0]  #could have no 0 values\\n    count_1 = 0 if 1 not in counts else counts[1]\\n    total = count_0 + count_1\\n    probs = (0,0) if total == 0 else (count_0/total, count_1/total)  #build 2-tuple\\n    return probs\\n\\ndef build_pred(column, branch):\\n    return lambda row: row[column] == branch\\n\\ndef find_best_splitter(table, choice_list, target):\\n  \\n    assert (len(table)>0),\"Cannot split empty table\"\\n    assert (target in table),\"Target must be column in table\"\\n    \\n    gig_scores = map(lambda col: (col, gig(table, col, target)), choice_list)  #compute tuple (col, gig) for each column\\n    gig_sorted = sorted(gig_scores, key=lambda item: item[1], reverse=True)  # sort on gig\\n    return gig_sorted\\n\\nfrom functools import reduce\\n\\ndef generate_table(table, conjunction):\\n  \\n    assert (len(table)>0),\"Cannot generate from empty table\"\\n\\n    sub_table = reduce(lambda subtable, pair: subtable.loc[pair[1]], conjunction, table)\\n    return sub_table\\n\\ndef compute_prediction(table, target):\\n  \\n    assert (len(table)>0),\"Cannot predict from empty table\"\\n    assert (target in table),\"Target must be column in table\"\\n    \\n    counts = table[target].value_counts()  # counts looks like {0: v1, 1: v2}\\n\\n    if 0 not in counts:\\n        prediction = 1\\n    elif 1 not in counts:\\n        prediction = 0\\n    elif counts[1] > counts[0]:  # ties go to 0 (negative)\\n        prediction = 1\\n    else:\\n        prediction = 0\\n\\n    return prediction\\n\\ndef build_tree_iter(table, choices, target, hypers={} ):\\n\\n    assert (len(choices)>0),\"Must have at least one column in choices\"\\n    assert (target in table), \"Target column not in table\"\\n    assert (len(table) > 1), \"Table must have more than 1 row\"\\n    \\n    k = hypers[\\'max-depth\\'] if \\'max-depth\\' in hypers else min(4, len(choices))\\n    gig_cutoff = hypers[\\'gig-cutoff\\'] if \\'gig-cutoff\\' in hypers else 0.0\\n    \\n    def iterative_build(k):\\n        columns_sorted = find_best_splitter(table, choices, target)\\n        (best_column, gig_value) = columns_sorted[0]\\n        \\n        #Note I add _1 or _0 to make it more readable for debugging\\n        current_paths = [{\\'conjunction\\': [(best_column+\\'_1\\', build_pred(best_column, 1))],\\n                          \\'prediction\\': None,\\n                          \\'gig_score\\': gig_value},\\n                         {\\'conjunction\\': [(best_column+\\'_0\\', build_pred(best_column, 0))],\\n                          \\'prediction\\': None,\\n                          \\'gig_score\\': gig_value}\\n                        ]\\n        k -= 1  # we just built a level as seed so subtract 1 from k\\n        tree_paths = []  # add completed paths here\\n        \\n        while k>0:\\n            new_paths = []\\n            for path in current_paths:\\n                old_conjunction = path[\\'conjunction\\']  # a list of (name, lambda)\\n                before_table = generate_table(table, old_conjunction)  #the subtable the current conjunct leads to\\n                columns_sorted = find_best_splitter(before_table, choices, target)\\n                (best_column, gig_value) = columns_sorted[0]\\n                if gig_value > gig_cutoff:\\n                    new_path_1 = {\\'conjunction\\': old_conjunction + [(best_column+\\'_1\\', build_pred(best_column, 1))],\\n                                \\'prediction\\': None,\\n                                 \\'gig_score\\': gig_value}\\n                    new_paths.append( new_path_1 ) #true\\n                    new_path_0 = {\\'conjunction\\': old_conjunction + [(best_column+\\'_0\\', build_pred(best_column, 0))],\\n                                \\'prediction\\': None,\\n                                 \\'gig_score\\': gig_value}\\n                    new_paths.append( new_path_0 ) #false\\n                else:\\n                    #not worth splitting so complete the path with a prediction\\n                    path[\\'prediction\\'] = compute_prediction(before_table, target)\\n                    tree_paths.append(path)\\n            #end for loop\\n            \\n            current_paths = new_paths\\n            if current_paths != []:\\n                k -= 1\\n            else:\\n                break  # nothing left to extend so have copied all paths to tree_paths\\n        #end while loop\\n\\n        #Generate predictions for all paths that have None\\n        for path in current_paths:\\n            conjunction = path[\\'conjunction\\']\\n            before_table = generate_table(table, conjunction)\\n            path[\\'prediction\\'] = compute_prediction(before_table, target)\\n            tree_paths.append(path)\\n        return tree_paths\\n\\n    return {\\'paths\\': iterative_build(k), \\'weight\\': None}\\n\\n\\ndef tree_predictor(row, tree):\\n    \\n    #go through each path, one by one (could use a map instead of for loop?)\\n    for path in tree[\\'paths\\']:\\n        conjuncts = path[\\'conjunction\\']\\n        result = map(lambda tuple: tuple[1](row), conjuncts)  # potential to be parallelized\\n        if all(result):\\n            return path[\\'prediction\\']\\n    raise LookupError(\\'No true paths found for row: \\' + str(row))\\n\\ndef path_id(row, tree):\\n    assert (len(tree[\\'paths\\']) > 0), \"Tree must have at least one path\"\\n    for path in tree[\\'paths\\']:\\n        conjuncts = path[\\'conjunction\\']\\n        result = map(lambda tuple: tuple[1](row), conjuncts)  # potential to be parallelized\\n        if all(result):\\n            return tree[\\'paths\\'].index(path)\\ndef reorder_paths(table, tree):\\n    pcount = table.apply(lambda row: path_id(row, tree), axis=1)\\n    test = pcount.value_counts()\\n    t = sorted(test.items(), key=lambda x: x[1], reverse=True)\\n    new_paths = []\\n    old = tree[\\'paths\\']\\n    for i,j in t:\\n        old1 = old[i]\\n        new_paths.append(old1)\\n\\n    return new_paths\\n\\ndef produce_scores(table, tree, target):\\n    scratch_table = pd.DataFrame(columns=[\\'prediction\\', \\'actual\\'])\\n    scratch_table[\\'prediction\\'] = table.apply(lambda row: tree_predictor(row, tree), axis=1)\\n    scratch_table[\\'actual\\'] = table[target]  # just copy the target column\\n    cases = scratch_table.apply(lambda row: predictor_case(row, pred=\\'prediction\\', target=\\'actual\\'), axis=1)\\n    vc = cases.value_counts()\\n    return [accuracy(vc), f1(vc), informedness(vc)]\\n\\ndef compute_training(slices, left_out):\\n    training_slices = []\\n    for i,slice in enumerate(slices):\\n        if i == left_out:\\n            continue\\n        training_slices.append(slices[i])\\n    return pd.concat(training_slices)  # note we are returning a table (DataFrame)\\n\\ndef k_fold(table, k, target, hypers, candidate_columns):\\n  \\n    #set up the table where we will record fold results\\n    result_columns = [\\'name\\',  \\'accuracy\\', \\'f1\\', \\'informedness\\']\\n    k_fold_results_table = pd.DataFrame(columns=result_columns)\\n    \\n    #generate the slices\\n    total_len = len(table.index)\\n    slice_size = int(total_len/(1.0*k))\\n    slices = []\\n    for i in range(k-1):\\n        a_slice =  table[i*slice_size:(i+1)*slice_size]\\n        slices.append( a_slice )\\n    slices.append( table[(k-1)*slice_size:] )  # whatever is left\\n    \\n    #generate test results\\n    all_scores = []  #keep track of all k results\\n    for i in range(k):\\n        test_table = slices[i]\\n        train_table = compute_training(slices, i)\\n        fold_tree = build_tree_iter(train_table, candidate_columns, target, hypers)  # train\\n        scores = produce_scores(test_table, fold_tree, target)  # test\\n        results_row = {\\'name\\': \\'fold_\\'+str(i), \\'accuracy\\': scores[0], \\'f1\\': scores[1], \\'informedness\\': scores[2]}\\n        k_fold_results_table = k_fold_results_table.append(results_row,ignore_index=True)\\n        all_scores.append(scores)\\n    \\n    #compute average of all folds\\n    avg_scores = tuple(reduce(lambda total, triple: np.add(triple, total), all_scores)/k)\\n    results_row = {\\'name\\': \\'average\\', \\'accuracy\\': avg_scores[0], \\'f1\\': avg_scores[1], \\'informedness\\': avg_scores[2]}\\n    k_fold_results_table = k_fold_results_table.append(results_row,ignore_index=True)\\n    \\n    #note that I add the meta comment as last step to avoid it being wiped out\\n    k_fold_results_table.meta = SimpleNamespace()\\n    k_fold_results_table.meta.hypers  = hypers # adds comment to remind me of hyper params used\\n    \\n    return k_fold_results_table\\n\\n#Determine if slices are mutually exclusive\\ndef verify_unique(slices):\\n    print((\\'total length all slices\\', sum([len(s) for s in slices])))\\n    for i, a_slice in enumerate(slices[:-1]):\\n        a_set = set(a_slice.index)\\n        for j, b_slice in enumerate(slices[i+1:]):\\n            b_set = set(b_slice.index)\\n            int_set = a_set.intersection(b_set)  # should be empty set as result\\n            print((i,j+i+1,int_set))\\n    return None\\n\\ndef k_fold_random(table, k, target, hypers, candidate_columns):\\n  #set up the table where we will record fold results\\n  result_columns = [\\'name\\',  \\'accuracy\\', \\'f1\\', \\'informedness\\']\\n  k_fold_results_table = pd.DataFrame(columns=result_columns)\\n\\n  #generate the slices\\n  table = shuffle(loan_table)\\n  total_len = len(table.index)\\n  split_size = int(total_len/(1.0*k))\\n  slices = []\\n\\n  #generate the slices\\n  for i in range(k-1):\\n    a_slice =  table[i*split_size:(i+1)*split_size]\\n    slices.append( a_slice )\\n  slices.append( table[(k-1)*split_size:] )\\n\\n  verify_unique(slices)  # should see 614 length and empty sets all the way down\\n\\n  #generate test results\\n  all_scores = []  #keep track of all k results\\n  for i in range(k):\\n      test_table = slices[i]\\n      train_table = compute_training(slices, i)\\n      fold_tree = build_tree_iter(train_table, candidate_columns, target, hypers)  # train\\n      scores = produce_scores(test_table, fold_tree, target)  # test\\n      results_row = {\\'name\\': \\'fold_\\'+str(i), \\'accuracy\\': scores[0], \\'f1\\': scores[1], \\'informedness\\': scores[2]}\\n      k_fold_results_table = k_fold_results_table.append(results_row,ignore_index=True)\\n      all_scores.append(scores)\\n\\n  #compute average of all folds\\n  avg_scores = tuple(reduce(lambda total, triple: np.add(triple, total), all_scores)/k)\\n  results_row = {\\'name\\': \\'average\\', \\'accuracy\\': avg_scores[0], \\'f1\\': avg_scores[1], \\'informedness\\': avg_scores[2]}\\n  k_fold_results_table = k_fold_results_table.append(results_row,ignore_index=True)\\n\\n  #note that I add the meta comment as last step to avoid it being wiped out\\n  k_fold_results_table.meta = SimpleNamespace()\\n  k_fold_results_table.meta.hypers  = hypers # adds comment to remind me of hyper params used\\n\\n  return k_fold_results_table\\n\\ndef vote_taker(row, forest):\\n    votes = {0:0, 1:0}\\n    for tree in forest:\\n        prediction = tree_predictor(row, tree)\\n        votes[prediction] += 1\\n    winner = 1 if votes[1]>votes[0] else 0  #ties go to 0\\n    return winner\\n\\ndef forest_scores(table, forest, target):\\n    scratch_table = pd.DataFrame(columns=[\\'prediction\\', \\'actual\\'])\\n    scratch_table[\\'prediction\\'] = table.apply(lambda row: vote_taker(row, forest), axis=1)  #only change is to call vote_taker\\n    scratch_table[\\'actual\\'] = table[target]  # just copy the target column\\n    cases = scratch_table.apply(lambda row: predictor_case(row, pred=\\'prediction\\', target=\\'actual\\'), axis=1)\\n    vc = cases.value_counts()\\n    return [accuracy(vc), f1(vc), informedness(vc)]\\n\\ndef forest_builder(table, column_choices, target, hypers):\\n\\n    tree_n = 5 if \\'total-trees\\' not in hypers else hypers[\\'total-trees\\']\\n    m = int(len(column_choices)**.5) if \\'m\\' not in hypers else hypers[\\'m\\']\\n    k = hypers[\\'max-depth\\'] if \\'max-depth\\' in hypers else min(2, len(column_choices))\\n    gig_cutoff = hypers[\\'gig-cutoff\\'] if \\'gig-cutoff\\' in hypers else 0.0\\n    rgen = hypers[\\'random-state\\'] if \\'random-state\\' in hypers else 0  #an int will work as seed with the sample method.\\n\\n    #build a single tree of depth n - call it multiple times to build multiple trees\\n    def iterative_build(n):\\n        train = table.sample(frac=1.0, replace=True, random_state=rgen)\\n        train = train.reset_index()\\n        left_out = table.loc[~table.index.isin(train[\\'index\\'])]\\n        left_out = left_out.reset_index() # this gives us the old index in its own column\\n        oob_list = left_out[\\'index\\'].tolist()  # list of row indices from original titanic table\\n        \\n        rcols = random.sample(column_choices, m)  # subspcace sampling - uses random.seed, not rng\\n        columns_sorted = find_best_splitter(train, rcols, target)\\n        (best_column, gig_value) = columns_sorted[0]\\n\\n        #Note I add _1 or _0 to make it more readable for debugging\\n        current_paths = [{\\'conjunction\\': [(best_column+\\'_1\\', build_pred(best_column, 1))],\\n                          \\'prediction\\': None,\\n                          \\'gig_score\\': gig_value},\\n                         {\\'conjunction\\': [(best_column+\\'_0\\', build_pred(best_column, 0))],\\n                          \\'prediction\\': None,\\n                          \\'gig_score\\': gig_value}\\n                        ]\\n        n -= 1  # we just built a level as seed so subtract 1 from n\\n        tree_paths = []  # add completed paths here\\n\\n        while n>0:\\n            new_paths = []\\n            for path in current_paths:\\n                conjunct = path[\\'conjunction\\']  # a list of (name, lambda)\\n                before_table = generate_table(train, conjunct)  #the subtable the current conjunct leads to\\n                rcols = random.sample(column_choices, m)  # subspace\\n                columns_sorted = find_best_splitter(before_table, rcols, target)\\n                (best_column, gig_value) = columns_sorted[0]\\n                if gig_value > gig_cutoff:\\n                    new_path_1 = {\\'conjunction\\': conjunct + [(best_column+\\'_1\\', build_pred(best_column, 1))],\\n                                \\'prediction\\': None,\\n                                 \\'gig_score\\': gig_value}\\n                    new_paths.append( new_path_1 ) #true\\n                    new_path_0 = {\\'conjunction\\': conjunct + [(best_column+\\'_0\\', build_pred(best_column, 0))],\\n                                \\'prediction\\': None,\\n                                 \\'gig_score\\': gig_value\\n                                 }\\n                    new_paths.append( new_path_0 ) #false\\n                else:\\n                    #not worth splitting so complete the path with a prediction\\n                    path[\\'prediction\\'] = compute_prediction(before_table, target)\\n                    tree_paths.append(path)\\n            #end for loop\\n\\n            current_paths = new_paths\\n            if current_paths != []:\\n                n -= 1\\n            else:\\n                break  # nothing left to extend so have copied all paths to tree_paths\\n        #end while loop\\n\\n        #Generate predictions for all paths that have None\\n        for path in current_paths:\\n            conjunct = path[\\'conjunction\\']\\n            before_table = generate_table(train, conjunct)\\n            path[\\'prediction\\'] = compute_prediction(before_table, target)\\n            tree_paths.append(path)\\n        return (tree_paths, oob_list)\\n    \\n    #let\\'s build a forest\\n    forest = []\\n    for i in range(tree_n):\\n        (paths, oob) = iterative_build(k)  #always use k for now\\n        forest.append({\\'paths\\': paths, \\'weight\\': None, \\'oob\\': oob})\\n        \\n    return forest\\n\\n\\ndef oob_table(forest):\\n\\to_list = []\\n\\tfor tree in forest:\\n\\t\\to_list += tree[\\'oob\\']\\n\\to_list = list(set(o_list))\\n\\ttesting_table = loan_table.loc[o_list]\\n\\ttesting_table = testing_table.reset_index()\\n\\treturn testing_table\\n\\ndef oob_vote_taker(row, forest):\\n\\tvotes = {0:0, 1:0}\\n\\tfor tree in forest:\\n\\t\\t# check to see if it is in oob\\n\\t\\tif row[\\'index\\'] in tree[\\'oob\\']:\\n\\t  \\t\\tprediction = tree_predictor(row, tree)\\n\\t  \\t\\tvotes[prediction] += 1\\n\\twinner = 1 if votes[1]>votes[0] else 0  #ties go to 0\\n\\treturn winner\\n\\ndef oob_forest_scores(table, forest, target):\\n\\tscratch_table = pd.DataFrame(columns=[\\'prediction\\', \\'actual\\'])\\n\\tscratch_table[\\'prediction\\'] = table.apply(lambda row: oob_vote_taker(row, forest), axis=1)\\n\\tscratch_table[\\'actual\\'] = table[target]  # just copy the target column\\n\\tcases = scratch_table.apply(lambda row: predictor_case(row, pred=\\'prediction\\', target=\\'actual\\'), axis=1)\\n\\tvc = cases.value_counts()\\n\\treturn [accuracy(vc), f1(vc), informedness(vc)]\\n'}"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"Il6HlbWggzMB","colab_type":"code","outputId":"9024c70c-c718-40d4-bc81-f74a7f167972","executionInfo":{"status":"ok","timestamp":1551240472719,"user_tz":480,"elapsed":369,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"cell_type":"code","source":["from library_w19_week6b import *\n","\n","%who function"],"execution_count":5,"outputs":[{"output_type":"stream","text":["accuracy\t build_pred\t build_tree_iter\t compute_prediction\t compute_training\t f1\t find_best_splitter\t forest_builder\t forest_scores\t \n","generate_table\t gig\t gini\t informedness\t k_fold\t k_fold_random\t oob_forest_scores\t oob_table\t oob_vote_taker\t \n","path_id\t predictor_case\t probabilities\t produce_scores\t reorder_paths\t tree_predictor\t verify_unique\t vote_taker\t \n"],"name":"stdout"}]},{"metadata":{"scrolled":true,"id":"benLXLswbgrp","colab_type":"code","outputId":"f8b20f8c-7a7e-416a-e6f7-0e43ae59e57b","executionInfo":{"status":"ok","timestamp":1551240475039,"user_tz":480,"elapsed":546,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"cell_type":"code","source":["pd.set_option('display.max_columns', None)\n","loan_table.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Gender</th>\n","      <th>Married</th>\n","      <th>Dependents</th>\n","      <th>Education</th>\n","      <th>Self_Employed</th>\n","      <th>ApplicantIncome</th>\n","      <th>CoapplicantIncome</th>\n","      <th>LoanAmount</th>\n","      <th>Loan_Amount_Term</th>\n","      <th>Credit_History</th>\n","      <th>Property_Area</th>\n","      <th>Loan_Status</th>\n","      <th>no_lam</th>\n","      <th>filled_lam</th>\n","      <th>pa_Rural</th>\n","      <th>pa_Semiurban</th>\n","      <th>pa_Urban</th>\n","      <th>pa_nan</th>\n","      <th>lam_bin</th>\n","      <th>lam_Low</th>\n","      <th>lam_Average</th>\n","      <th>lam_High</th>\n","      <th>ch_bad</th>\n","      <th>ch_good</th>\n","      <th>ch_nan</th>\n","      <th>apin_binned</th>\n","      <th>apin_low</th>\n","      <th>apin_average</th>\n","      <th>apin_high</th>\n","      <th>apin_nan</th>\n","      <th>dep_0</th>\n","      <th>dep_1</th>\n","      <th>dep_2</th>\n","      <th>dep_3+</th>\n","      <th>dep_nan</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>0</td>\n","      <td>Graduate</td>\n","      <td>No</td>\n","      <td>5849</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>360.0</td>\n","      <td>1.0</td>\n","      <td>Urban</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>146.412162</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Male</td>\n","      <td>Yes</td>\n","      <td>1</td>\n","      <td>Graduate</td>\n","      <td>No</td>\n","      <td>4583</td>\n","      <td>1508.0</td>\n","      <td>128.0</td>\n","      <td>360.0</td>\n","      <td>1.0</td>\n","      <td>Rural</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>128.000000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Male</td>\n","      <td>Yes</td>\n","      <td>0</td>\n","      <td>Graduate</td>\n","      <td>Yes</td>\n","      <td>3000</td>\n","      <td>0.0</td>\n","      <td>66.0</td>\n","      <td>360.0</td>\n","      <td>1.0</td>\n","      <td>Urban</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>66.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Male</td>\n","      <td>Yes</td>\n","      <td>0</td>\n","      <td>Not Graduate</td>\n","      <td>No</td>\n","      <td>2583</td>\n","      <td>2358.0</td>\n","      <td>120.0</td>\n","      <td>360.0</td>\n","      <td>1.0</td>\n","      <td>Urban</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>120.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Male</td>\n","      <td>No</td>\n","      <td>0</td>\n","      <td>Graduate</td>\n","      <td>No</td>\n","      <td>6000</td>\n","      <td>0.0</td>\n","      <td>141.0</td>\n","      <td>360.0</td>\n","      <td>1.0</td>\n","      <td>Urban</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>141.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>low</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Gender Married Dependents     Education Self_Employed  ApplicantIncome  \\\n","0   Male      No          0      Graduate            No             5849   \n","1   Male     Yes          1      Graduate            No             4583   \n","2   Male     Yes          0      Graduate           Yes             3000   \n","3   Male     Yes          0  Not Graduate            No             2583   \n","4   Male      No          0      Graduate            No             6000   \n","\n","   CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n","0                0.0         NaN             360.0             1.0   \n","1             1508.0       128.0             360.0             1.0   \n","2                0.0        66.0             360.0             1.0   \n","3             2358.0       120.0             360.0             1.0   \n","4                0.0       141.0             360.0             1.0   \n","\n","  Property_Area  Loan_Status  no_lam  filled_lam  pa_Rural  pa_Semiurban  \\\n","0         Urban            1       1  146.412162         0             0   \n","1         Rural            0       0  128.000000         1             0   \n","2         Urban            1       0   66.000000         0             0   \n","3         Urban            1       0  120.000000         0             0   \n","4         Urban            1       0  141.000000         0             0   \n","\n","   pa_Urban  pa_nan lam_bin  lam_Low  lam_Average  lam_High  ch_bad  ch_good  \\\n","0         1       0     Low        1            0         0       0        1   \n","1         0       0     Low        1            0         0       0        1   \n","2         1       0     Low        1            0         0       0        1   \n","3         1       0     Low        1            0         0       0        1   \n","4         1       0     Low        1            0         0       0        1   \n","\n","   ch_nan apin_binned  apin_low  apin_average  apin_high  apin_nan  dep_0  \\\n","0       0         low         1             0          0         0      1   \n","1       0         low         1             0          0         0      0   \n","2       0         low         1             0          0         0      1   \n","3       0         low         1             0          0         0      1   \n","4       0         low         1             0          0         0      1   \n","\n","   dep_1  dep_2  dep_3+  dep_nan  \n","0      0      0       0        0  \n","1      1      0       0        0  \n","2      0      0       0        0  \n","3      0      0       0        0  \n","4      0      0       0        0  "]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"b13zz-YpspVE","colab_type":"code","outputId":"5ddcdb52-223b-44e4-8b20-c7b3f3c44477","executionInfo":{"status":"ok","timestamp":1551240475781,"user_tz":480,"elapsed":467,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":317}},"cell_type":"code","source":["loan_table.describe()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ApplicantIncome</th>\n","      <th>CoapplicantIncome</th>\n","      <th>LoanAmount</th>\n","      <th>Loan_Amount_Term</th>\n","      <th>Credit_History</th>\n","      <th>Loan_Status</th>\n","      <th>no_lam</th>\n","      <th>filled_lam</th>\n","      <th>pa_Rural</th>\n","      <th>pa_Semiurban</th>\n","      <th>pa_Urban</th>\n","      <th>pa_nan</th>\n","      <th>lam_Low</th>\n","      <th>lam_Average</th>\n","      <th>lam_High</th>\n","      <th>ch_bad</th>\n","      <th>ch_good</th>\n","      <th>ch_nan</th>\n","      <th>apin_low</th>\n","      <th>apin_average</th>\n","      <th>apin_high</th>\n","      <th>apin_nan</th>\n","      <th>dep_0</th>\n","      <th>dep_1</th>\n","      <th>dep_2</th>\n","      <th>dep_3+</th>\n","      <th>dep_nan</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>592.000000</td>\n","      <td>600.00000</td>\n","      <td>564.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.0</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.0</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","      <td>614.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>5403.459283</td>\n","      <td>1621.245798</td>\n","      <td>146.412162</td>\n","      <td>342.00000</td>\n","      <td>0.842199</td>\n","      <td>0.687296</td>\n","      <td>0.035831</td>\n","      <td>146.412162</td>\n","      <td>0.291531</td>\n","      <td>0.379479</td>\n","      <td>0.328990</td>\n","      <td>0.0</td>\n","      <td>0.905537</td>\n","      <td>0.074919</td>\n","      <td>0.019544</td>\n","      <td>0.144951</td>\n","      <td>0.773616</td>\n","      <td>0.081433</td>\n","      <td>0.988599</td>\n","      <td>0.008143</td>\n","      <td>0.003257</td>\n","      <td>0.0</td>\n","      <td>0.561889</td>\n","      <td>0.166124</td>\n","      <td>0.164495</td>\n","      <td>0.083062</td>\n","      <td>0.024430</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>6109.041673</td>\n","      <td>2926.248369</td>\n","      <td>85.587325</td>\n","      <td>65.12041</td>\n","      <td>0.364878</td>\n","      <td>0.463973</td>\n","      <td>0.186019</td>\n","      <td>84.037468</td>\n","      <td>0.454838</td>\n","      <td>0.485653</td>\n","      <td>0.470229</td>\n","      <td>0.0</td>\n","      <td>0.292710</td>\n","      <td>0.263475</td>\n","      <td>0.138540</td>\n","      <td>0.352339</td>\n","      <td>0.418832</td>\n","      <td>0.273722</td>\n","      <td>0.106250</td>\n","      <td>0.089945</td>\n","      <td>0.057026</td>\n","      <td>0.0</td>\n","      <td>0.496559</td>\n","      <td>0.372495</td>\n","      <td>0.371027</td>\n","      <td>0.276201</td>\n","      <td>0.154506</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>150.000000</td>\n","      <td>0.000000</td>\n","      <td>9.000000</td>\n","      <td>12.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>9.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>2877.500000</td>\n","      <td>0.000000</td>\n","      <td>100.000000</td>\n","      <td>360.00000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>100.250000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>3812.500000</td>\n","      <td>1188.500000</td>\n","      <td>128.000000</td>\n","      <td>360.00000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>129.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>5795.000000</td>\n","      <td>2297.250000</td>\n","      <td>168.000000</td>\n","      <td>360.00000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>164.750000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>81000.000000</td>\n","      <td>41667.000000</td>\n","      <td>700.000000</td>\n","      <td>480.00000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>700.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n","count       614.000000         614.000000  592.000000         600.00000   \n","mean       5403.459283        1621.245798  146.412162         342.00000   \n","std        6109.041673        2926.248369   85.587325          65.12041   \n","min         150.000000           0.000000    9.000000          12.00000   \n","25%        2877.500000           0.000000  100.000000         360.00000   \n","50%        3812.500000        1188.500000  128.000000         360.00000   \n","75%        5795.000000        2297.250000  168.000000         360.00000   \n","max       81000.000000       41667.000000  700.000000         480.00000   \n","\n","       Credit_History  Loan_Status      no_lam  filled_lam    pa_Rural  \\\n","count      564.000000   614.000000  614.000000  614.000000  614.000000   \n","mean         0.842199     0.687296    0.035831  146.412162    0.291531   \n","std          0.364878     0.463973    0.186019   84.037468    0.454838   \n","min          0.000000     0.000000    0.000000    9.000000    0.000000   \n","25%          1.000000     0.000000    0.000000  100.250000    0.000000   \n","50%          1.000000     1.000000    0.000000  129.000000    0.000000   \n","75%          1.000000     1.000000    0.000000  164.750000    1.000000   \n","max          1.000000     1.000000    1.000000  700.000000    1.000000   \n","\n","       pa_Semiurban    pa_Urban  pa_nan     lam_Low  lam_Average    lam_High  \\\n","count    614.000000  614.000000   614.0  614.000000   614.000000  614.000000   \n","mean       0.379479    0.328990     0.0    0.905537     0.074919    0.019544   \n","std        0.485653    0.470229     0.0    0.292710     0.263475    0.138540   \n","min        0.000000    0.000000     0.0    0.000000     0.000000    0.000000   \n","25%        0.000000    0.000000     0.0    1.000000     0.000000    0.000000   \n","50%        0.000000    0.000000     0.0    1.000000     0.000000    0.000000   \n","75%        1.000000    1.000000     0.0    1.000000     0.000000    0.000000   \n","max        1.000000    1.000000     0.0    1.000000     1.000000    1.000000   \n","\n","           ch_bad     ch_good      ch_nan    apin_low  apin_average  \\\n","count  614.000000  614.000000  614.000000  614.000000    614.000000   \n","mean     0.144951    0.773616    0.081433    0.988599      0.008143   \n","std      0.352339    0.418832    0.273722    0.106250      0.089945   \n","min      0.000000    0.000000    0.000000    0.000000      0.000000   \n","25%      0.000000    1.000000    0.000000    1.000000      0.000000   \n","50%      0.000000    1.000000    0.000000    1.000000      0.000000   \n","75%      0.000000    1.000000    0.000000    1.000000      0.000000   \n","max      1.000000    1.000000    1.000000    1.000000      1.000000   \n","\n","        apin_high  apin_nan       dep_0       dep_1       dep_2      dep_3+  \\\n","count  614.000000     614.0  614.000000  614.000000  614.000000  614.000000   \n","mean     0.003257       0.0    0.561889    0.166124    0.164495    0.083062   \n","std      0.057026       0.0    0.496559    0.372495    0.371027    0.276201   \n","min      0.000000       0.0    0.000000    0.000000    0.000000    0.000000   \n","25%      0.000000       0.0    0.000000    0.000000    0.000000    0.000000   \n","50%      0.000000       0.0    1.000000    0.000000    0.000000    0.000000   \n","75%      0.000000       0.0    1.000000    0.000000    0.000000    0.000000   \n","max      1.000000       0.0    1.000000    1.000000    1.000000    1.000000   \n","\n","          dep_nan  \n","count  614.000000  \n","mean     0.024430  \n","std      0.154506  \n","min      0.000000  \n","25%      0.000000  \n","50%      0.000000  \n","75%      0.000000  \n","max      1.000000  "]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"-tU8vHMzqIX0","colab_type":"text"},"cell_type":"markdown","source":["<h1>You are mostly on your own</h1>\n","\n","I am going to give you the freedom to implement the K-NN components. My only constraints are that you implement these functions:\n","<pre>\n","def euclidean_distance(vector1, vector2):  #euclidean distance between 2 vectors\n","\n","def knn(row_index, table, k, columns, target):  #provide prediction for row given K-NN algorithm. Only use values in columns.\n","\n","def knn_tester(table, k, columns, target): #produce predictions for every row in table according to columns. Return accuracy score. Danger: O(n**2).\n","</pre>\n"]},{"metadata":{"id":"Yy9DuIfYqH9l","colab_type":"text"},"cell_type":"markdown","source":["I am calling these \"splitter\" columns but that is a misnomer. Splitter has to do with decision trees. Should rename them at some point."]},{"metadata":{"id":"AVGS40Subgrv","colab_type":"code","colab":{}},"cell_type":"code","source":["splitter_columns = [\n","        #Dependents\n","        'dep_0', 'dep_1', 'dep_2', 'dep_3+',\n","        #ApplicantIncome\n","       'ApplicantIncome', 'CoapplicantIncome',\n","        #Property_Area\n","        'pa_Rural', 'pa_Semiurban','pa_Urban',\n","        #LoanAmount\n","        'filled_lam',\n","        #Credit_History\n","        'ch_bad', 'ch_good']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LxLfsWIgbgr0","colab_type":"code","colab":{}},"cell_type":"code","source":["target = 'Loan_Status'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LBx1F-7Tuf0-","colab_type":"code","colab":{}},"cell_type":"code","source":["#your code\n","import math\n","import operator\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","#euclidean distance between 2 vectors\n","def euclidean_distance(vector1, vector2):\n","  distance = math.sqrt(sum([(a - b) ** 2 for a, b in zip(vector1, vector2)]))\n","  return distance\n","  \n","#provide prediction for row given K-NN algorithm. Only use values in columns.\n","def knn(row_index, table, columns, k, target):\n","  dist = []\n","  rows = loan_table[columns].iloc[row_index].tolist()\n","  for i, row in loan_table.iterrows():\n","    if i == row_index:\n","      continue\n","    dist.append((i,euclidean_distance(rows, row[columns].tolist())))\n","  dist.sort(key=operator.itemgetter(1))\n","  vote = []\n","  for (i,j) in dist[:k]:\n","    vote.append(loan_table[target].iloc[i])\n","  return max(set(vote), key=vote.count)\n","\n","#produce predictions for every row in table according to columns. Return accuracy score. Danger: O(n**2).\n","def knn_tester(table, k, columns, target):\n","  all_votes = []\n","  for i, row in table.iterrows():\n","    all_votes.append(knn(i, table, columns, k, target))\n","  table['all_votes'] = all_votes\n","  table['vote_type'] = table.apply(lambda row: predictor_case(row, \n","  pred='all_votes', target=target), axis=1)\n","  p1_types = table['vote_type'].value_counts()\n","  return accuracy(p1_types)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qwBgmhNHaNUp","colab_type":"text"},"cell_type":"markdown","source":["<h2>Match my results</h2>\n","\n","Start with single predictions."]},{"metadata":{"id":"31qPdpGQvAEV","colab_type":"code","outputId":"c204cf19-2486-47ed-ec21-8f4a4d6cb6e2","executionInfo":{"status":"ok","timestamp":1551240483228,"user_tz":480,"elapsed":803,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["#test with row 1, k=5\n","import time\n","\n","start = time.time()\n","    \n","prediction_row0 = knn(1, loan_table,splitter_columns,5, target)        \n","print('prediction: ' + str(prediction_row0))  #actual is 0\n","end = time.time()\n","print('elapsed time: '+ str(end - start))  # in seconds"],"execution_count":11,"outputs":[{"output_type":"stream","text":["prediction: 1\n","elapsed time: 0.44704627990722656\n"],"name":"stdout"}]},{"metadata":{"id":"6vzyStkOanX1","colab_type":"code","outputId":"a7321aff-9a58-4186-9f9c-907146532c5a","executionInfo":{"status":"ok","timestamp":1551240484357,"user_tz":480,"elapsed":713,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["#test with row 1, k=11\n","\n","import time\n","\n","start = time.time()\n","    \n","prediction_row0 = knn(1, loan_table,splitter_columns, 11, target)        \n","print('prediction: ' + str(prediction_row0))\n","end = time.time()\n","print('elapsed time: '+ str(end - start))  # in seconds"],"execution_count":12,"outputs":[{"output_type":"stream","text":["prediction: 1\n","elapsed time: 0.45952796936035156\n"],"name":"stdout"}]},{"metadata":{"id":"DaPDdFBVbTL0","colab_type":"text"},"cell_type":"markdown","source":["<h2>Now to testing</h2>\n","\n","knn_tester should use knn to make predictions for every row. Check predictions against target and compute and return accuracy.\n","<p>\n","All of my functions are nothing fancy. And it costs me about 2 minutes per run."]},{"metadata":{"id":"mxaVrxbP0QL7","colab_type":"code","outputId":"485dc459-e049-4122-f211-df3055bfc9b6","executionInfo":{"status":"ok","timestamp":1551240761867,"user_tz":480,"elapsed":272535,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["#k=5 (about 2 minutes on colab)\n","\n","start = time.time()\n","    \n","the_accuracy = knn_tester(loan_table, 5, splitter_columns, target)        \n","print('accuracy: ' + str(the_accuracy))\n","end = time.time()\n","print('elapsed time: '+ str(end - start))  # in seconds"],"execution_count":13,"outputs":[{"output_type":"stream","text":["accuracy: 0.6302931596091205\n","elapsed time: 272.10260939598083\n"],"name":"stdout"}]},{"metadata":{"id":"L6YXMl6F2Aw8","colab_type":"text"},"cell_type":"markdown","source":["<h2>Not that great</h2>\n","\n","Why so bad? Normally K-NN fails when there are too many features/columns in play. It relies on distances, and the more columns you add, the harder to discern distance differences. This is an example of the curse of dimensionality (jargon alert). It is kind of counter-intuitive: the more information (features/columns) you add, the worse you do. Here is an intro to the problem: http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/. Let's reduce our columns and see if it makes any difference.\n"]},{"metadata":{"id":"A8-Fhupy21ZD","colab_type":"code","colab":{}},"cell_type":"code","source":["fewer_columns = [\n","        #ApplicantIncome\n","       'ApplicantIncome', 'CoapplicantIncome',\n","        #LoanAmount\n","        'filled_lam',\n","        #Credit_History\n","        'ch_bad', 'ch_good']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hch84l713FrD","colab_type":"code","outputId":"fff3d49d-8445-403e-bb33-1528ac5c1687","executionInfo":{"status":"ok","timestamp":1551241395117,"user_tz":480,"elapsed":267999,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["#k=5 (about 2 minutes on colab)\n","\n","start = time.time()\n","    \n","the_accuracy = knn_tester(loan_table, 5, fewer_columns, target)        \n","print('accuracy: ' + str(the_accuracy))\n","end = time.time()\n","print('elapsed time: '+ str(end - start))  # in seconds"],"execution_count":15,"outputs":[{"output_type":"stream","text":["accuracy: 0.6302931596091205\n","elapsed time: 267.61279249191284\n"],"name":"stdout"}]},{"metadata":{"id":"6kN_5-mr2um0","colab_type":"text"},"cell_type":"markdown","source":["Exactly the same. Let's try some different values for k."]},{"metadata":{"id":"r_rGy04_5-l4","colab_type":"code","outputId":"da683297-f829-4ea2-9827-f1a9deb44e56","executionInfo":{"status":"ok","timestamp":1551241763970,"user_tz":480,"elapsed":272957,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["#k = 11, full columns\n","start = time.time()\n","    \n","the_accuracy = knn_tester(loan_table, 11, splitter_columns, target)        \n","print('accuracy: ' + str(the_accuracy))\n","end = time.time()\n","print('elapsed time: '+ str(end - start))  # in seconds"],"execution_count":16,"outputs":[{"output_type":"stream","text":["accuracy: 0.6628664495114006\n","elapsed time: 272.42414450645447\n"],"name":"stdout"}]},{"metadata":{"id":"ZsFrmQrQ6yTz","colab_type":"text"},"cell_type":"markdown","source":["Got a bit of improvement. One more."]},{"metadata":{"id":"2KgVOEnK7n4s","colab_type":"code","outputId":"c923fd75-b6a3-4582-d569-55dee781dee3","executionInfo":{"status":"ok","timestamp":1551242453332,"user_tz":480,"elapsed":273064,"user":{"displayName":"Nicholas Bonat","photoUrl":"https://lh5.googleusercontent.com/-Q-bFxRhEAJo/AAAAAAAAAAI/AAAAAAAAAFM/FPsWH90byZ0/s64/photo.jpg","userId":"04391020366496854291"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["#k = 3, full columns\n","start = time.time()\n","    \n","the_accuracy = knn_tester(loan_table, 3, splitter_columns, target)        \n","print('accuracy: ' + str(the_accuracy))\n","end = time.time()\n","print('elapsed time: '+ str(end - start))  # in seconds"],"execution_count":17,"outputs":[{"output_type":"stream","text":["accuracy: 0.6042345276872965\n","elapsed time: 272.4772951602936\n"],"name":"stdout"}]},{"metadata":{"id":"y4hNVpR183_y","colab_type":"text"},"cell_type":"markdown","source":["Heading back down in accuracy. I'll stop here but will leave you with an extra credit assignment (optional)."]},{"metadata":{"id":"c0y5ncFNk1IX","colab_type":"text"},"cell_type":"markdown","source":["<h2>Update your library</h2>\n","\n","Name it `library_w19_week7.py`."]},{"metadata":{"id":"Cr5Q-oG09HbX","colab_type":"text"},"cell_type":"markdown","source":["<h2>Extra Credit</h2>\n","\n","Beat my testing times. You can use any of the pandas and numpy methods. As reminder, here were my times (approximately) using brute force approach:\n","<pre>\n","knn_tester(loan_table, 5, splitter_columns, target) : 107ish\n","knn_tester(loan_table, 5, fewer_columns, target) :  83ish\n","knn_tester(loan_table, 11, splitter_columns, target) : 108ish\n","knn_tester(loan_table, 3, splitter_columns, target) : 108ish\n","</pre>\n","<p>\n","  Here is a time I got by a fairly simple trick:\n","  <pre>\n","  knn_tester(loan_table, 3, splitter_columns, target) \n","  accuracy: 0.6042345276872965\n","elapsed time: 4.287370681762695\n","</pre>\n","So from 108 to 4. Pretty dang good."]}]}